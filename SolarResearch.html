<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Cold Regions Research and Engineering Lab Solar Panel Imaging Project</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#section-TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#section-TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="section-TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Stefano Fochesatto</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="course.html">Course Work</a>
</li>
<li>
  <a href="research.html">Research</a>
</li>
<li>
  <a href="talks.html">Speaking</a>
</li>
<li>
  <a href="software.html">Programming</a>
</li>
<li>
  <a href="blog.html">Blog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="./files/Stefano_Resume.pdf">
    <span class="ai ai-cv ai-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/StefanoFochesatto">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/stefano-fochesatto-076447213/">
    <span class="fab fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="section-header">



<h1 class="title toc-ignore">Cold Regions Research and Engineering Lab
Solar Panel Imaging Project</h1>

</div>


<p><link rel="stylesheet" href="styles.css" type="text/css">
<link rel="stylesheet" href="academicicons/css/academicons.min.css"/></p>
<div id="section-overview" class="section level1">
<h1>Overview</h1>
<p>The goal of this project is to develop a workflow which extracts high
quality snow cover data from time-lapse videos of solar panels. This
project has undergone many iterations and approaches. Initially we had
devised an algorithm which extracted snow cover data using only computer
vision techniques, like <a
href="https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html">Otsu
Binary Thresholding</a>, <a
href="https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html">HSV
Color Thresholding</a>, and <a
href="https://docs.opencv.org/3.4/d4/d13/tutorial_py_filtering.html">various
filtering methods</a>. As you’ll see this algorithm did no produce snow
cover data with high enough quality and these initial measurements were
too susceptible to noise from glare and varying lighting conditions even
after applying several noise reduction, and outlier detection techniques
like Kernel Density Smoothing and <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html">Isolation
Forest</a>. This initial algorithm was however incredibly effective as a
machine assisted labeling tool to quickly create training data for the
deep learning segmentation model which was eventually used. 7,000+ high
quality training pairs were created using the initial computer vision
algorithm. This training data was used to train a <a
href="https://arxiv.org/pdf/1512.03385.pdf">ResNet50</a>-<a
href="https://arxiv.org/pdf/1707.03718.pdf">Linknet</a> segmentation
model. The model can accurately segment the snow from each panel giving
us high quality, low noise snow cover data.</p>
</div>
<div id="section-methods" class="section level1">
<h1>Methods</h1>
<div id="section-localizing-the-solar-panels" class="section level2">
<h2>Localizing the Solar Panels</h2>
<p>With the current camera and solar panel setup, throughout the
time-lapse videos the position of the solar panels in the frame changes
over time. We discussed ways in which we could adjust the design of the
experiment to allow the retrieval of the data without moving the
camera.</p>
<p>To address this the current setup, has two of the three legs which
support the camera attached to the frame of the solar panel array. This
should have the effect of moving both the camera and panel array
together, keeping the panel array in the same spot of the frame.</p>
<p>Generally we observed that within each video a single mask can be
used to localize the solar panels. Our solution, which is employed in
both the Computer Vision Algorithm and the Segmentation Model Algorithm
is to display the first frame of each video, and have the user manually
input the location of the solar panels.</p>
<p><img src="CRRELReportImages/PanelLocalization.jpg" style="width:100%" align="center"></p>
<p><img src="CRRELReportImages/PanelLocalizationMarked.jpg" style="width:100%" align="center"></p>
<p>Eventually I know we might be able to use this method to create
training data for a landmark detection or <a
href="https://github.com/ultralytics/yolov5">object detection</a>
algorithm.</p>
</div>
<div id="section-time-stamp-extraction" class="section level2">
<h2>Time Stamp Extraction</h2>
<p>Unfortunately, metadata from the videos did not contain any real-time
clock information. Therefore the only way to extract the timestamps was
from the video itself. To do so we used the <a
href="https://pypi.org/project/easyocr/">EasyOCR python library</a>. The
first two timestamps of every video were cropped, resized, and padded
then <a
href="https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.htmls">dilation,
erosion</a>, and median filters were applied. The preprocessed timestamp
images were then run through the EasyOCR engine and converted to
datetime objects. Subtracting them we can compute the time step
automatically. With the time step and initial timestamp computed the
rest can be found by addition. This method allows our technicians to
change the time step without having to adjust anything in the
software.</p>
<p><img src="CRRELReportImages/Crop.png" style="width:100%" align="center"></p>
<p><img src="CRRELReportImages/CropPreprocess.png" style="width:100%" align="center"></p>
</div>
<div id="section-machine-assisted-labeling" class="section level2">
<h2>Machine Assisted Labeling</h2>
<div id="section-computer-vision-algorithm" class="section level3">
<h3>Computer Vision Algorithm</h3>
<p>With the panels localized, and timestamps extracted we can finally
move on to the image processing. As what was previously discussed the
initial algorithm relied entirely on computer vision techniques. The
following is a generalized recipe for the algorithm.</p>
<ul>
<li>Take a small sample of the sky and compute the mean intensity from
this sample.</li>
<li>Crop out the solar panel</li>
<li>Apply user generated mask</li>
<li>Apply median blur (night time dots from up lighting)</li>
<li>Apply bilateral filter (remove daytime vertical lines from down
lighting)</li>
<li>Compute HSV Threshold, range is dependent on sample intensity</li>
<li>Compute Otsu Threshold value (A custom function was written which
counts the number of black border pixels, and removes them before the
Otsu Threshold is calculated)
<ul>
<li>Mean window with a kernel size of 6 frames was used to impute any
outlier thresholds that exceed half a standard deviation.</li>
</ul></li>
<li><a
href="https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.normalized_root_mse">Normalized
Root Mean Squared Error</a> is computed between the two segmentation
methods (HSV and Otsu). This method computes dissimilarity between the
two segmentation methods on a scale from 0 to 1. When the value is 1 the
resultant segmentation are completely dissimilar.
<ul>
<li>If the score is greater than or equal .90, we compute the snow cover
percentage for both segmentation methods and compare them to a the mean
snow cover over the last 5 frames. We use which ever segmentation
results in a snow cover reading that is closer to the mean.</li>
</ul></li>
</ul>
<p>This method had several issues. In the transitions between day and
night we see the color balance of the camera change, which causes huge
issues with the HSV thresholding. This method also cannot account for
glare, so any time the panels are in direct sunlight with no snow on the
panel it will register any reflections as snow. Below is an example of
what happens when the panels are in direct sunlight.</p>
<p><img src="CRRELReportImages/ComputerVisionAlgo.png" style="width:100%" align="center"></p>
<p>Beyond the image processing, the script which contains this algorithm
also does all the data collation to compile the snow cover data into a
single .csv file. It can also generate a debugging video, for quickly
scrubbing through and finding the timestamps where the algorithm did a
poor job at extracting data.</p>
</div>
<div id="section-cleaning-training-data" class="section level3">
<h3>Cleaning Training Data</h3>
<p>Clearly this method had a lot of issues to overcome, so we pivoted to
a deep learning approach. Luckily the computer vision algorithm was
still incredibly useful as a machine assisted labeling tool for
generating the training data that is needed for our deep learning model.
Running the original computer vision algorithm again, but saving the
masked version of the initial crop and the final segmentation we get the
following training data.</p>
<p><img src="CRRELReportImages/Trainingdata.jpg" style="width:100%" align="center"></p>
<p>Since each frame computes 4 different training pairs, at a time step
of frame per minute we can approximate that a single time lapse video
which lasts a period of 24 hours will generate a total of 5,760 training
pairs.</p>
<p><span class="math display">\[ \frac{4\text{ Training Pairs}}{1 \text{
Frame}}*\frac{1 \text{ Frame}}{1 \text{ Minute}}*\frac{60 \text{
Minutes}}{1 \text{ Hour}}*\frac{24 \text{ Hours}}{1 \text{ Time Lapse}}
= \frac{5,760\text{ Training Pairs}}{1 \text{ Time Lapse}}\]</span></p>
<p>We found that the fastest way to scrub through the data was to use
the Mac OS finder or Adobe Bridge to display a grid of training pairs.
Here is an example of how one might format the finder, or Adobe Bridge
widow to scrub the images</p>
<p><img src="CRRELReportImages/CleaningData.jpg" style="width:100%" align="center"></p>
<p>To scrub the images we simply deleted the pairs that exhibited noise.
The following an example of a training pair which the computer vision
algorithm chose HSV thresholding, and the coatings on this particular
panel reflect a color that is being confused for snow.</p>
<p><img src="CRRELReportImages/PoorPair.png" style="width:100%" align="center"></p>
<p>Large sections of training images that were exposed to direct glare
were instead blacked out, so as to train the segmentation network to
ignore direct sunlight. In the future we think it would be even better
to encode these images into a separate class and train the network to
detect the presence of glare as well as snow, instead of just ignoring
it.</p>
<p><img src="CRRELReportImages/PhotoshopExample.png" style="width:100%" align="center"></p>
</div>
</div>
<div id="section-deep-learning-segmentation-model"
class="section level2">
<h2>Deep Learning Segmentation Model</h2>
<p>Using only two videos, we were able to generate 7,000+ high quality
training pairs with instances of high snow cover and glare in a matter
of hours. With the training data generated we used the <a
href="https://github.com/qubvel/segmentation_models">Segmentation Models
python library</a> to easily test different network architectures and
backbones. As an aside, generally we refer to a network backbone as the
encoder or feature extractor. This is the section of the network that is
compressing the images, using the loss function and back-propagation to
guide which features get extracted from the image. The network
architecture, at least in this context, defines how the layers of the
network are connected. Consider the difference between a U-net
architecture and a Link-net architecture.</p>
<p><img src="CRRELReportImages/NetworkArchitectureExample.jpg" style="width:100%" align="center"></p>
<p>We can see that in the U-net, layers from the encoder or backbone are
concatenated with the layers from the decoder. In the Link-net those
same layers are added instead. This generally has the effect of reducing
the number of parameters in the model, while maintaining
performance.</p>
<p>For our model we used a ResNet50 backbone and a Linknet architecture,
below is a high level sketch of what it looks like.</p>
<p><img src="CRRELReportImages/OurModel.jpg" style="width:100%" align="center"></p>
<p>The model was trained using a combination of the <a
href="https://arxiv.org/pdf/1708.02002.pdf">binary focal</a> and the <a
href="https://arxiv.org/pdf/2006.14822.pdf">dice loss</a> functions. The
dice loss function optimizes a similar metric to IoU. The binary focal
loss function allows us to weigh certain classes that we might feel are
underrepresented in the data, using a hyper parameter <span
class="math inline">\(\gamma\)</span>. In our model, both the non-snow
and snow classes were weighted evenly, but this will also allow us to
add a glare class later on, which will likely be underrepresented in our
data. Let <span class="math inline">\(p_t\)</span> be the predicted
probability for a pixel in class <span class="math inline">\(t\)</span>,
and let <span class="math inline">\(y\)</span> be the true value (0 or
1).</p>
<p><span class="math display">\[Loss(y, p_t) = FL(p_t) + DL(y +
p_t)\]</span> <span class="math display">\[Loss(y, p_t) = -(1 -
p_t)^{\gamma}log(p_t) + 1 - \frac{2yp_t + 1}{2y + p_t + 1}\]</span></p>
</div>
</div>
<div id="section-results" class="section level1">
<h1>Results</h1>
<p>The performance of segmentation models is usually measured with a
metric called IOU, or intersection over union. Basically we divide the
intersection between our true and predicted mask, and divide that by the
sum or union of both masks. Below is an graphic which illustrates this
idea.</p>
<p><img src="CRRELReportImages/IOU.png" style="width:100%" align="center"></p>
<p>On the IoU metric our model achieved a final value of .8986. Thanks
to the segmentation models library we were able to take advantage of
pre-trained weights trained on the ImageNet dataset. Because of this
from the first epoch our model achieved an IoU score of .8749.</p>
<p><img src="CRRELReportImages/IOUChart.png" style="width:100%" align="center"></p>
<p>We can get a better sense of the performance of our model by plotting
the snow cover percentage over time. Here we can see that the
segmentation model is more robust to direct sunlight, and changes in
lighting conditions.</p>
<p><img src="CRRELReportImages/SnowCoverMeasurementComparisonEdit.png" style="width:100%" align="center"></p>
</div>
<div id="section-conclusions" class="section level1">
<h1>Conclusions</h1>
<p>There are still several things we can do to improve the performance
of the segmentation model, and the user experience for our technicians.
For example, developing an object or landmark detection model for
localizing the solar panels, parallelizing the image processing step to
increase performance, and retraining the segmentation model to include
glare as a third class. Outside of the software realm, changing the
camera settings so that white balance is set to a constant value,
instead of auto will make it easier for the computer vision algorithm to
generate pairs and it will improve our segmentation model’s accuracy.
Currently we cannot support a time step that is smaller than one minute,
and with the current solar panel localization approach, increasing the
time step to multiple days or even hours might cause the masks to become
misaligned.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#section-TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
